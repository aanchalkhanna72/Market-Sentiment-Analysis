{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a66d9d",
   "metadata": {},
   "source": [
    "# CAPSTONE DATA COLLECTION PROCESS (MSI)\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9e54c",
   "metadata": {},
   "source": [
    "*Thank you for helping us out. We appreciate it!*\n",
    "\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb295887",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS:\n",
    "\n",
    "#### STEP 1: Download the ipynb file from your inbox. Do not rename it. You will also find an excel file in the email with your assigned tasks (by way of serial numbers). Open this file in another tab (you will need it later).\n",
    "#### STEP 2: Locate the ipynb file and place it on the desktop. \n",
    "#### STEP 3: Create a folder on the desktop. Name it \"CAPSTONE MSI\".\n",
    "#### STEP 4: Save the ipynb file in the new folder and launch it on jupyter notebook.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c18585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aanchalkhanna72/Desktop'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting current working directory - it should have \"...desktop/CAPSTONE MSI\" in it\n",
    "#\n",
    "#\n",
    "#\n",
    "import os\n",
    "os.chdir('/Users/aanchalkhanna72/Desktop/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7db10",
   "metadata": {},
   "source": [
    "#### STEP 5: Run the 1st cell to  import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087f972e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b107",
   "metadata": {},
   "source": [
    "#### STEP 6: Run the 2nd and 3rd cells. Input your serial number. Be very careful to not alter any code. Your output in cell 2 should correspond with  your first task in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1661a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Halaarav/CapStone/main/List.csv')\n",
    "def serial(serial_number):\n",
    "    temp = data.loc[data['Serial_Number'] == serial_number]\n",
    "    year = int(temp.Year)\n",
    "    month = int(temp.Month)\n",
    "    start = int(temp.R1)\n",
    "    end = int(temp.R2)\n",
    "    return [year,month,start,end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaaf57",
   "metadata": {},
   "source": [
    "# THIS IS THE MOST IMPORTANT STEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a8a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your serial number in the brackets with (****) and run this cell.\n",
    "#\n",
    "#\n",
    "#\n",
    "year,month,start,end = serial(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8843d9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2019, 5, 43586, 43617)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year,month,start,end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903e345",
   "metadata": {},
   "source": [
    "#### STEP 7: Run the 4th cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40cbe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell \n",
    "#\n",
    "#\n",
    "#\n",
    "#Given url for month, find links of all days published and store in the \"day_links\" variable, all articles in \"articles_links\" variable\n",
    "#\n",
    "#\n",
    "#\n",
    "#creating a loop for all articles to be scraped for each day in the month \n",
    "day_links=[]\n",
    "\n",
    "for i in range(int(start),int(end)):\n",
    "\n",
    "    #considering the url of month of 'august' that increments by 1, this loop will scrape articles for all days in the month of August\n",
    "    day_link=\"https://economictimes.indiatimes.com/archivelist/year-\"+str(year)+\",month-\"+ str(month)+\",starttime-\"+str(i)+\".cms\"\n",
    "    day_links.append(day_link)    \n",
    "\n",
    "def transform(day_link):\n",
    "        \n",
    "        #function to request browser to get information from given web page\n",
    "        def get_url(url):\n",
    "            return requests.get(url, {'headers':headers})\n",
    "        \n",
    "        #specifies the header/user agent,ie, any software that retrieves and presents Web content for end users or is implemented using Web technologies. User agents include Web browsers, media players, and plug-ins that help in retrieving, rendering and interacting with Web content.\n",
    "        headers={\n",
    "            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15'\n",
    "        }\n",
    "        \n",
    "        #creating empty dictionary where scraped data will be stored\n",
    "        dict_day = {'date_published':[],'date_updated':[],'time':[],\n",
    "                             'headline':[],'sector/category':[],'synopsis':[],'full_text':[]}\n",
    "        \n",
    "        #creates a list (not set) of all links scraped from archive of 1 day \n",
    "        article_links = []\n",
    "        \n",
    "        #creating 1st soup oject\n",
    "        soup1=bs4.BeautifulSoup(get_url(day_link).text,'html.parser') #creates soup object for 1 day archive traversal\n",
    "        #https://economictimes.indiatimes.com/archivelist/year-2008,month-1,starttime-39448.cms\n",
    "\n",
    "        #date is scraped from the day page itself instead of going into individual news links\n",
    "        try:\n",
    "            date_pub=soup1.find_all('b')[1].text\n",
    "        except:\n",
    "            date_pub='NA'\n",
    "\n",
    "        #collecting links of all news articles published on a day\n",
    "        for ultag in soup1.find_all('ul', class_= 'content'): #looks for tag 'ul',class='content' under which links are present\n",
    "            for litag in ultag.find_all('li'):\n",
    "                for atag in litag.find_all('a'):\n",
    "                    if(atag.get('href')!='#'):\n",
    "                        link_reqd = \"https://economictimes.indiatimes.com\" + atag.get('href')\n",
    "                        article_links.append(link_reqd)\n",
    "\n",
    "\n",
    "        #Begins scraping content that needs to be written in the file\n",
    "\n",
    "        #Scrapes article level data from links collected for the day stored in all_links\n",
    "        for links in article_links: \n",
    "\n",
    "            #published date is appended as many times as there are number of article links for the day\n",
    "            dict_day['date_published'].append(date_pub)\n",
    "\n",
    "            #creating soup object of each link so that we can traverse through each news article\n",
    "            soup=bs4.BeautifulSoup(get_url(links).text,features='html.parser')\n",
    "\n",
    "            #scrapes headline, synopsis, date_updated, time, sector, full_text from respective HTML class \n",
    "            try:\n",
    "                dict_day['headline'].append(soup.find('h1').text)\n",
    "            except:\n",
    "                dict_day['headline'].append('NA')\n",
    "\n",
    "            #to prevent errors in non-existent updated dates/different synopsis class names\n",
    "            try:\n",
    "                dict_day['synopsis'].append(soup.find('h2',class_=\"summary\").text)\n",
    "            except:\n",
    "                dict_day['synopsis'].append('NA')\n",
    "\n",
    "            try:\n",
    "                dict_day['date_updated'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[2:5]])) #first splits elements, then merges them to give date for last updated \n",
    "            except:\n",
    "                dict_day['date_updated'].append('NA')\n",
    "\n",
    "            try:   \n",
    "                dict_day['time'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[5:8]])) #first splits elements, then merges them to give time for last updated\n",
    "            except:\n",
    "                dict_day['time'].append('NA')\n",
    "\n",
    "            try:\n",
    "                dict_day['sector/category'].append(links.rsplit('/')[4:6]) #gets sectors/category from the url itself\n",
    "            except:\n",
    "                dict_day['sector/category'].append('NA')\n",
    "                \n",
    "            try:\n",
    "                try:\n",
    "                    partial_text=soup.find('article',class_='artData clr').text\n",
    "                    all_text=partial_text[:partial_text.find(\"Experience Your Economic Times\")-21]\n",
    "                    dict_day['full_text'].append(all_text)\n",
    "\n",
    "                except:\n",
    "                    some_text=soup.find('article',class_='artData clr paywall').text\n",
    "                    final_text=some_text[:some_text.find(\"Experience Your Economic Times\")-23]\n",
    "                    dict_day['full_text'].append(final_text)\n",
    "                    #partial_text_json=soup.find_all('script',type=\"application/ld+json\")[1].get_text()\n",
    "                    #all_text_json=partial_text_json[partial_text_json.find(\"articleBody\")+14:partial_text_json.find(\"image\")-15]\n",
    "                    #dict_day['full_text'].append(all_text_json)\n",
    "            \n",
    "            except AttributeError:\n",
    "                dict_day['full_text'].append('NA')\n",
    "        \n",
    "        #convert dictionary to a dataframe so as to save as csv\n",
    "        df=pd.DataFrame(dict_day)\n",
    "        #print(dict_day)\n",
    "\n",
    "        #file name is unique as it's linked to unique article id visible in the url\n",
    "        file_name=date_pub\n",
    "\n",
    "        #Saving File as 'article id'.csv\n",
    "        df.to_csv('./{}.csv'.format(file_name), sep=',')\n",
    "\n",
    "        #empty dictionary and list\n",
    "        dict_day.clear()\n",
    "        article_links.clear()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        return (\"Download done.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47b382",
   "metadata": {},
   "source": [
    "#### STEP 8: Run the 5th cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44a332",
   "metadata": {},
   "source": [
    "# DO NOT INTERRUPT WHILE IT IS RUNNING!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c44986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "#parallelizing operations\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "#start=timer()\n",
    "#with ThreadPoolExecutor() as executor:\n",
    "   # try:\n",
    "#     for result in executor.map(transform,day_links):\n",
    " #           print(result)\n",
    "  #  except:\n",
    "   #     print(\"A task failed!\")\n",
    "#end=timer()\n",
    "#print(((end-start)/60),\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec90a1ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode characters in position 2518-2521: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/var/folders/rm/vj2l2dln2m1_8v73jz4v60gm0000gn/T/ipykernel_13232/3430577464.py\", line 118, in transform\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 3466, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\", line 1105, in to_csv\n    csv_formatter.save()\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\", line 257, in save\n    self._save()\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\", line 262, in _save\n    self._save_body()\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\", line 300, in _save_body\n    self._save_chunk(start_i, end_i)\n  File \"/Users/aanchalkhanna72/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\", line 311, in _save_chunk\n    libwriters.write_csv_rows(\n  File \"pandas/_libs/writers.pyx\", line 55, in pandas._libs.writers.write_csv_rows\nUnicodeEncodeError: 'utf-8' codec can't encode characters in position 2518-2521: surrogates not allowed\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rm/vj2l2dln2m1_8v73jz4v60gm0000gn/T/ipykernel_13232/4262503835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m res = Parallel(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )(delayed(transform)(x) for x in day_links)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'utf-8' codec can't encode characters in position 2518-2521: surrogates not allowed"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "start=timer()\n",
    "res = Parallel(\n",
    "    n_jobs=-1\n",
    ")(delayed(transform)(x) for x in day_links)\n",
    "print(res)\n",
    "end=timer()\n",
    "print(((end-start)/60),\"mins\")\n",
    "#alerts when code is done\n",
    "#£duration = 3  # seconds\n",
    "#freq = 440  # Hz\n",
    "#os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5560c4",
   "metadata": {},
   "source": [
    "#### STEP 9: Keep checking your desktop folder to see if the files (saved as \"DATE-MONTH-YEAR.csv\") are getting downloaded. Once the kernel is running, please ask one of us to take a look. DO NOT  INTERRUPT THE KERNEL.\n",
    "#### STEP 10: Once the first task is COMPLETE AND YOU SEE A TIME STAMP in the last kernel, leave out the ipynb file and combine the remaining files in a new folder named \"MONTH-YEAR\". Zip this folder.\n",
    "#### STEP 11: SUBMIT THIS ZIPPED FOLDER. \n",
    "#### STEP 12:  Repeat all steps from 5-11 in this notebook. Now, save the new folder named \"(new) MONTH-YEAR\". Remove ipynb file from folder. Zip the folder with combined months.\n",
    "### SUBMIT!!!  YOU ARE DONE. THANK YOU!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a97335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
