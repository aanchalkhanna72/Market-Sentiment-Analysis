{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a66d9d",
   "metadata": {},
   "source": [
    "# CAPSTONE DATA COLLECTION PROCESS (MSI)\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9e54c",
   "metadata": {},
   "source": [
    "*Thank you for helping us out. We appreciate it!*\n",
    "\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb295887",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS:\n",
    "\n",
    "#### STEP 1: Download the ipynb file from your inbox. Do not rename it. You will also find an excel file in the email with your assigned tasks (by way of serial numbers). Open this file in another tab (you will need it later).\n",
    "#### STEP 2: Locate the ipynb file and place it on the desktop. \n",
    "#### STEP 3: Create a folder on the desktop. Name it \"CAPSTONE MSI\".\n",
    "#### STEP 4: Save the ipynb file in the new folder and launch it on jupyter notebook.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c18585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aanchalkhanna72/Desktop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting current working directory - it should have \"...desktop/CAPSTONE MSI\" in it\n",
    "#\n",
    "#\n",
    "#\n",
    "import os\n",
    "os.chdir('/Users/aanchalkhanna72/Desktop/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7db10",
   "metadata": {},
   "source": [
    "#### STEP 5: Run the 1st cell to  import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087f972e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b107",
   "metadata": {},
   "source": [
    "#### STEP 6: Run the 2nd and 3rd cells. Input your serial number. Be very careful to not alter any code. Your output in cell 2 should correspond with  your first task in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1661a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Halaarav/CapStone/main/List.csv')\n",
    "def serial(serial_number):\n",
    "    temp = data.loc[data['Serial_Number'] == serial_number]\n",
    "    year = int(temp.Year)\n",
    "    month = int(temp.Month)\n",
    "    start = int(temp.R1)\n",
    "    end = int(temp.R2)\n",
    "    return [year,month,start,end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaaf57",
   "metadata": {},
   "source": [
    "# THIS IS THE MOST IMPORTANT STEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a8a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your serial number in the brackets with (****) and run this cell.\n",
    "#\n",
    "#\n",
    "#\n",
    "year,month,start,end = serial(176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8843d9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2008, 11, 39753, 39783)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year,month,start,end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903e345",
   "metadata": {},
   "source": [
    "#### STEP 7: Run the 4th cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40cbe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell \n",
    "#\n",
    "#\n",
    "#\n",
    "#Given url for month, find links of all days published and store in the \"day_links\" variable, all articles in \"articles_links\" variable\n",
    "#\n",
    "#\n",
    "#\n",
    "#creating a loop for all articles to be scraped for each day in the month \n",
    "day_links=[]\n",
    "\n",
    "for i in range(int(start),int(end)):\n",
    "\n",
    "    #considering the url of month of 'august' that increments by 1, this loop will scrape articles for all days in the month of August\n",
    "    day_link=\"https://economictimes.indiatimes.com/archivelist/year-\"+str(year)+\",month-\"+ str(month)+\",starttime-\"+str(i)+\".cms\"\n",
    "    day_links.append(day_link)    \n",
    "\n",
    "def transform(day_link):\n",
    "        \n",
    "        #function to request browser to get information from given web page\n",
    "        def get_url(url):\n",
    "            return requests.get(url, {'headers':headers})\n",
    "        \n",
    "        #specifies the header/user agent,ie, any software that retrieves and presents Web content for end users or is implemented using Web technologies. User agents include Web browsers, media players, and plug-ins that help in retrieving, rendering and interacting with Web content.\n",
    "        headers={\n",
    "            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15'\n",
    "        }\n",
    "        \n",
    "        #creating empty dictionary where scraped data will be stored\n",
    "        dict_day = {'date_published':[],'date_updated':[],'time':[],\n",
    "                             'headline':[],'sector/category':[],'synopsis':[],'full_text':[]}\n",
    "        \n",
    "        #creates a list (not set) of all links scraped from archive of 1 day \n",
    "        article_links = []\n",
    "        \n",
    "        #creating 1st soup oject\n",
    "        soup1=bs4.BeautifulSoup(get_url(day_link).text,'html.parser') #creates soup object for 1 day archive traversal\n",
    "        #https://economictimes.indiatimes.com/archivelist/year-2008,month-1,starttime-39448.cms\n",
    "\n",
    "        #date is scraped from the day page itself instead of going into individual news links\n",
    "        try:\n",
    "            date_pub=soup1.find_all('b')[1].text\n",
    "        except:\n",
    "            date_pub='NA'\n",
    "\n",
    "        #collecting links of all news articles published on a day\n",
    "        for ultag in soup1.find_all('ul', class_= 'content'): #looks for tag 'ul',class='content' under which links are present\n",
    "            for litag in ultag.find_all('li'):\n",
    "                for atag in litag.find_all('a'):\n",
    "                    if(atag.get('href')!='#'):\n",
    "                        link_reqd = \"https://economictimes.indiatimes.com\" + atag.get('href')\n",
    "                        article_links.append(link_reqd)\n",
    "\n",
    "\n",
    "        #Begins scraping content that needs to be written in the file\n",
    "\n",
    "        #Scrapes article level data from links collected for the day stored in all_links\n",
    "        for links in article_links: \n",
    "\n",
    "            #published date is appended as many times as there are number of article links for the day\n",
    "            dict_day['date_published'].append(date_pub)\n",
    "\n",
    "            #creating soup object of each link so that we can traverse through each news article\n",
    "            soup=bs4.BeautifulSoup(get_url(links).text,features='html.parser')\n",
    "\n",
    "            #scrapes headline, synopsis, date_updated, time, sector, full_text from respective HTML class \n",
    "            try:\n",
    "                dict_day['headline'].append(soup.find('h1').text)\n",
    "            except:\n",
    "                dict_day['headline'].append('NA')\n",
    "\n",
    "            #to prevent errors in non-existent updated dates/different synopsis class names\n",
    "            try:\n",
    "                dict_day['synopsis'].append(soup.find('h2',class_=\"summary\").text)\n",
    "            except:\n",
    "                dict_day['synopsis'].append('NA')\n",
    "\n",
    "            try:\n",
    "                dict_day['date_updated'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[2:5]])) #first splits elements, then merges them to give date for last updated \n",
    "            except:\n",
    "                dict_day['date_updated'].append('NA')\n",
    "\n",
    "            try:   \n",
    "                dict_day['time'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[5:8]])) #first splits elements, then merges them to give time for last updated\n",
    "            except:\n",
    "                dict_day['time'].append('NA')\n",
    "\n",
    "            try:\n",
    "                dict_day['sector/category'].append(links.rsplit('/')[4:6]) #gets sectors/category from the url itself\n",
    "            except:\n",
    "                dict_day['sector/category'].append('NA')\n",
    "                \n",
    "            try:\n",
    "                try:\n",
    "                    partial_text=soup.find('article',class_='artData clr').text\n",
    "                    all_text=partial_text[:partial_text.find(\"Experience Your Economic Times\")-21]\n",
    "                    dict_day['full_text'].append(all_text)\n",
    "\n",
    "                except:\n",
    "                    some_text=soup.find('article',class_='artData clr paywall').text\n",
    "                    final_text=some_text[:some_text.find(\"Experience Your Economic Times\")-23]\n",
    "                    dict_day['full_text'].append(final_text)\n",
    "                    #partial_text_json=soup.find_all('script',type=\"application/ld+json\")[1].get_text()\n",
    "                    #all_text_json=partial_text_json[partial_text_json.find(\"articleBody\")+14:partial_text_json.find(\"image\")-15]\n",
    "                    #dict_day['full_text'].append(all_text_json)\n",
    "            \n",
    "            except AttributeError:\n",
    "                dict_day['full_text'].append('NA')\n",
    "        \n",
    "        #convert dictionary to a dataframe so as to save as csv\n",
    "        df=pd.DataFrame(dict_day)\n",
    "        #print(dict_day)\n",
    "\n",
    "        #file name is unique as it's linked to unique article id visible in the url\n",
    "        file_name=date_pub\n",
    "\n",
    "        #Saving File as 'article id'.csv\n",
    "        df.to_csv('./{}.csv'.format(file_name), sep=',')\n",
    "\n",
    "        #empty dictionary and list\n",
    "        dict_day.clear()\n",
    "        article_links.clear()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        return (\"Download done.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fb32a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39753.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39754.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39755.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39756.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39757.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39758.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39759.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39760.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39761.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39762.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39763.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39764.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39765.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39766.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39767.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39768.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39769.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39770.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39771.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39772.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39773.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39774.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39775.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39776.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39777.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39778.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39779.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39780.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39781.cms',\n",
       " 'https://economictimes.indiatimes.com/archivelist/year-2008,month-11,starttime-39782.cms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47b382",
   "metadata": {},
   "source": [
    "#### STEP 8: Run the 5th cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44a332",
   "metadata": {},
   "source": [
    "# DO NOT INTERRUPT WHILE IT IS RUNNING!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c44986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "#\n",
    "#\n",
    "#\n",
    "#parallelizing operations\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "#start=timer()\n",
    "#with ThreadPoolExecutor() as executor:\n",
    "   # try:\n",
    "#     for result in executor.map(transform,day_links):\n",
    " #           print(result)\n",
    "  #  except:\n",
    "   #     print(\"A task failed!\")\n",
    "#end=timer()\n",
    "#print(((end-start)/60),\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec90a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.', 'Download done.']\n",
      "30.495880791216667 mins\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "start=timer()\n",
    "res = Parallel(\n",
    "    n_jobs=-1\n",
    ")(delayed(transform)(x) for x in day_links)\n",
    "print(res)\n",
    "end=timer()\n",
    "print(((end-start)/60),\"mins\")\n",
    "#alerts when code is done\n",
    "#Â£duration = 3  # seconds\n",
    "#freq = 440  # Hz\n",
    "#os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5560c4",
   "metadata": {},
   "source": [
    "#### STEP 9: Keep checking your desktop folder to see if the files (saved as \"DATE-MONTH-YEAR.csv\") are getting downloaded. Once the kernel is running, please ask one of us to take a look. DO NOT  INTERRUPT THE KERNEL.\n",
    "#### STEP 10: Once the first task is COMPLETE AND YOU SEE A TIME STAMP in the last kernel, leave out the ipynb file and combine the remaining files in a new folder named \"MONTH-YEAR\". Zip this folder.\n",
    "#### STEP 11: SUBMIT THIS ZIPPED FOLDER. \n",
    "#### STEP 12:  Repeat all steps from 5-11 in this notebook. Now, save the new folder named \"(new) MONTH-YEAR\". Remove ipynb file from folder. Zip the folder with combined months.\n",
    "### SUBMIT!!!  YOU ARE DONE. THANK YOU!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a97335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
