{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84b62c7",
   "metadata": {},
   "source": [
    "## 2 - Web Scraping for 1 Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9893b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d3796",
   "metadata": {},
   "source": [
    "#### Saving articles for 1 Day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6eb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to request browser to get information\n",
    "def get_url(url):\n",
    "   return requests.get(url, {'headers':headers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6ae396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifies the header/user agent,ie, any software that retrieves and presents Web content for end users or is implemented using Web technologies. User agents include Web browsers, media players, and plug-ins that help in retrieving, rendering and interacting with Web content.\n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15'\n",
    "}\n",
    "\n",
    "#creating empty dictionary where scraped data will be stored\n",
    "dict_day = {'date_published':[],'date_updated':[],'time':[],\n",
    "                     'headline':[],'sector/category':[],'synopsis':[],'full_text':[]}\n",
    "\n",
    "#creates a list (not set) of all links scraped from archive of 1 day \n",
    "article_links = []\n",
    "\n",
    "#creating 1st soup oject\n",
    "soup1=bs4.BeautifulSoup(get_url(\"https://economictimes.indiatimes.com/archivelist/year-2022,month-5,starttime-44682.cms\").text,'html.parser') #creates soup object for 1 day archive traversal\n",
    "#https://economictimes.indiatimes.com/archivelist/year-2008,month-1,starttime-39448.cms\n",
    "\n",
    "#date is scraped from the day page itself instead of going into individual news links\n",
    "try:\n",
    "    date_pub=soup1.find_all('b')[1].text\n",
    "except:\n",
    "    date_pub='NA'\n",
    "\n",
    "#collecting links of all news articles published on a day\n",
    "for ultag in soup1.find_all('ul', class_= 'content'): #looks for tag 'ul',class='content' under which links are present\n",
    "    for litag in ultag.find_all('li'):\n",
    "        for atag in litag.find_all('a'):\n",
    "            if(atag.get('href')!='#'):\n",
    "                link_reqd = \"https://economictimes.indiatimes.com\" + atag.get('href')\n",
    "                article_links.append(link_reqd)\n",
    "\n",
    "def transform(article_links):\n",
    "    \n",
    "    start=timer()\n",
    "\n",
    "    #Begins scraping content that needs to be written in the file\n",
    "\n",
    "    #Scrapes article level data from links collected for the day stored in all_links\n",
    "    for links in article_links: \n",
    "\n",
    "        #published date is appended as many times as there are number of article links for the day\n",
    "        dict_day['date_published'].append(date_pub)\n",
    "\n",
    "        #creating soup object of each link so that we can traverse through each news article\n",
    "        soup=bs4.BeautifulSoup(get_url(links).text,features='html.parser')\n",
    "\n",
    "        #scrapes headline, synopsis, date_updated, time, sector, full_text from respective HTML class \n",
    "        try:\n",
    "            dict_day['headline'].append(soup.find('h1').text)\n",
    "        except:\n",
    "            dict_day['headline'].append('NA')\n",
    "\n",
    "        #to prevent errors in non-existent updated dates/different synopsis class names\n",
    "        try:\n",
    "            dict_day['synopsis'].append(soup.find('h2',class_=\"summary\").text)\n",
    "        except:\n",
    "            dict_day['synopsis'].append('NA')\n",
    "\n",
    "        try:\n",
    "            dict_day['date_updated'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[2:5]])) #first splits elements, then merges them to give date for last updated \n",
    "        except:\n",
    "            dict_day['date_updated'].append('NA')\n",
    "\n",
    "        try:   \n",
    "            dict_day['time'].append(\" \".join([str(item) for item in soup.find('time',class_=\"jsdtTime\").text.rsplit(' ')[5:8]])) #first splits elements, then merges them to give time for last updated\n",
    "        except:\n",
    "            dict_day['time'].append('NA')\n",
    "\n",
    "        try:\n",
    "            dict_day['sector/category'].append(links.rsplit('/')[4:6]) #gets sectors/category from the url itself\n",
    "        except:\n",
    "            dict_day['sector/category'].append('NA')\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                partial_text=soup.find('article',class_='artData clr').text\n",
    "                all_text=partial_text[:partial_text.find(\"Experience Your Economic Times\")-21]\n",
    "                dict_day['full_text'].append(all_text)\n",
    "\n",
    "            except:\n",
    "                some_text=soup.find('article',class_='artData clr paywall').text\n",
    "                final_text=some_text[:some_text.find(\"Experience Your Economic Times\")-23]\n",
    "                dict_day['full_text'].append(final_text)\n",
    "                #partial_text_json=soup.find_all('script',type=\"application/ld+json\")[1].get_text()\n",
    "                #all_text_json=partial_text_json[partial_text_json.find(\"articleBody\")+14:partial_text_json.find(\"image\")-15]\n",
    "                #dict_day['full_text'].append(all_text_json)\n",
    "\n",
    "        except AttributeError:\n",
    "            dict_day['full_text'].append('NA')\n",
    "\n",
    "    #convert dictionary to a dataframe so as to save as csv\n",
    "    df=pd.DataFrame(dict_day)\n",
    "    #print(dict_day)\n",
    "\n",
    "    #file name is unique as it's linked to unique article id visible in the url\n",
    "    file_name=date_pub\n",
    "\n",
    "    #Saving File as 'article id'.csv\n",
    "    #df.to_csv('./{}.csv'.format(file_name), sep=',')\n",
    "    print(df)\n",
    "\n",
    "    end=timer()\n",
    "\n",
    "    #print(f'\\nTime to complete: {end-start:.2f}s\\n')\n",
    "    return(\"Start:\",start,\"End:\",end,\"Time taken: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b6bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date_published   date_updated          time  \\\n",
      "0      1 May, 2022             NA            NA   \n",
      "1      1 May, 2022             NA            NA   \n",
      "2      1 May, 2022             NA            NA   \n",
      "3      1 May, 2022             NA            NA   \n",
      "4      1 May, 2022             NA            NA   \n",
      "..             ...            ...           ...   \n",
      "185    1 May, 2022  May 01, 2022,  11:26 PM IST   \n",
      "186    1 May, 2022  May 02, 2022,  09:47 AM IST   \n",
      "187    1 May, 2022  May 01, 2022,  11:33 PM IST   \n",
      "188    1 May, 2022  May 02, 2022,  12:11 PM IST   \n",
      "189    1 May, 2022  May 02, 2022,  12:02 AM IST   \n",
      "\n",
      "                                              headline  \\\n",
      "0    Startups, tech firms fuel demand for coworking...   \n",
      "1                                             404 page   \n",
      "2    Innovation in the 5G space will lead to newer ...   \n",
      "3                                             404 page   \n",
      "4    Startups, tech firms fuel demand for coworking...   \n",
      "..                                                 ...   \n",
      "185            Cricket: CSK beat SRH by 13 runs in IPL   \n",
      "186  Power Minister RK Singh to meet states, lender...   \n",
      "187  N Sankar used ethics, integrity and trust as c...   \n",
      "188  Tata chief N Chandrasekaran's desert safari lo...   \n",
      "189  European nations key companions in India's que...   \n",
      "\n",
      "                                       sector/category  \\\n",
      "0    [startups, startups-tech-firms-fuel-demand-for...   \n",
      "1    [ettech-briefs, telcos-regulator-on-different-...   \n",
      "2    [technology, innovation-in-the-5g-space-will-l...   \n",
      "3    [ettech-briefs, nokia-ready-to-jointly-develop...   \n",
      "4    [technology, startups-tech-firms-fuel-demand-f...   \n",
      "..                                                 ...   \n",
      "185       [sports, ipl-csk-beat-srh-by-13-runs-in-ipl]   \n",
      "186                                    [energy, power]   \n",
      "187  [india, n-sankar-used-ethics-integrity-and-tru...   \n",
      "188                        [company, corporate-trends]   \n",
      "189  [india, european-nations-key-companions-in-ind...   \n",
      "\n",
      "                                              synopsis  \\\n",
      "0                                                   NA   \n",
      "1                                                   NA   \n",
      "2                                                   NA   \n",
      "3                                                   NA   \n",
      "4                                                   NA   \n",
      "..                                                 ...   \n",
      "185  Nicholas Pooran top-scored for SRH with an unb...   \n",
      "186  Industry sources say peak demand is expected t...   \n",
      "187  N Ram, Director, The Hindu Publishing Group sa...   \n",
      "188  ADQ, Mubadala make investments in Tata Motors ...   \n",
      "189  PM Modi is expected to emphasise India's appro...   \n",
      "\n",
      "                                             full_text  \n",
      "0                                                   NA  \n",
      "1                                                   NA  \n",
      "2                                                   NA  \n",
      "3                                                   NA  \n",
      "4                                                   NA  \n",
      "..                                                 ...  \n",
      "185  PTIDevon Conway and Ruturaj Gaikwad of Chennai...  \n",
      "186  PTIMeeting comes amid 134 BU of electricity de...  \n",
      "187  AgenciesN Sankar There was a full house at the...  \n",
      "188  Thoughts swirled in Natarajan Chandrasekaran’s...  \n",
      "189  AgenciesThe Prime Minister will make clear Ind...  \n",
      "\n",
      "[190 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Start:',\n",
       " 544.839676412,\n",
       " 'End:',\n",
       " 645.202680748,\n",
       " 'Time taken: ',\n",
       " 100.36300433600002)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08153219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820bf16",
   "metadata": {},
   "source": [
    "## Documentation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7af04",
   "metadata": {},
   "source": [
    "Experiment 2: Automating  process to retrieve articles for one month \n",
    "\n",
    "Contributors: Aanchal\n",
    "\n",
    "Aim: Implement a sequential loop to retrieve articles for one month \n",
    "\n",
    "Dependencies: Beautiful Soup, Timer \n",
    "\n",
    "Method: \n",
    "We created two functions to: \n",
    "a) make url requests, \n",
    "b) create a dataframe of all articles for a day and save it as one day’s csv file \n",
    "\n",
    "(b) was the basis on  which all our further codes were built. Its pseudo-code was as follows:\n",
    "Create an empty dictionary to store all info about day’s articles : \n",
    "dict_day = {'date_published':[ ],'date_updated':[ ],'time':[ ],\n",
    "                       'Headline':[ ],'sector/category':[ ],'synopsis':[ ],'full_text':[ ]}\n",
    "\n",
    "Create  an empty list to store article links:\n",
    " article_links = list() \n",
    "\n",
    "Initialize the user agent (web browser used to request information)\n",
    "Create first BeautifulSoup object \n",
    "Extract date published from the  day’s page itself instead of repeatedly scraping it from each article one by one \n",
    "Do:\n",
    "For each tag under 'ul',class='content':\n",
    "Go into ‘li’ class\n",
    "Go into ‘a’ class\n",
    "If ‘href’ attribute is not null, then concatenate its content to the page url to give  the final page link\n",
    "Append each link to ‘articles_links’\n",
    "Until: All article links in the day page are appended \n",
    "Def transform(article_links):\n",
    "Do: \n",
    "Record start time \n",
    "For each link in the list argument:\n",
    "Record published date obtained in (5)\n",
    "Create a soup object that parses the HTML link \n",
    "Try:\n",
    " to obtain headline, synopsis, date\n",
    "updated, sector/category/full_text\n",
    "\t\t\tExcept:\n",
    " Append ‘NA’ to the respective column\n",
    "      9. Until: All links in ‘article_links’ are parsed \n",
    "     10. Convert dictionary to  dataframe \n",
    "     11. Save file with name as published date of articles\n",
    "     12. Print dataframe \n",
    "     13. Record End time\n",
    "     13. Return: time taken for operation\n",
    "\n",
    "Observations:\n",
    "Our previous challenge was to find patterns in full text class names so that we could envision building a uniform code for all articles for all 178 months. We came over this problem by implementing a ‘try/except’ block for two classes, namely ‘artData clr' (for non-ET prime articles) and ‘artData clr paywall' (for ET prime articles). We used indexing to remove unnecessary textual information/marketing text and append the final full_text to the dictionary. \n",
    "\n",
    "During this phase,  we found that  the processing time of one day’s articles was high. When this operation would be performed for a month (tiems 178), it would take very long to scrape all required articles for the project.\n",
    "\n",
    "Conclusion:\n",
    "So as to scrape 350-500 articles for each day of the month, we needed to fasten the process as each day’s articles downloading time was high at 15 mins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e1854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
